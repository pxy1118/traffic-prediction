# ğŸš¦ PEMS08 äº¤é€šæµé¢„æµ‹ç³»ç»Ÿ

åŸºäºå›¾å·ç§¯ç½‘ç»œ(GCN)ç‰¹å¾æå–å’Œæœºå™¨å­¦ä¹ çš„å¤šæ­¥äº¤é€šæµé¢„æµ‹ç³»ç»Ÿï¼Œæ”¯æŒå¤šç§é¢„æµ‹æ¨¡å‹å’Œäº¤äº’å¼å¯è§†åŒ–ã€‚

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®ä½¿ç”¨PEMS-08æ•°æ®é›†ï¼ˆ170ä¸ªäº¤é€šä¼ æ„Ÿå™¨èŠ‚ç‚¹ï¼‰è¿›è¡Œäº¤é€šæµé¢„æµ‹ï¼Œé‡‡ç”¨"GCNå…¨å±€ç‰¹å¾æå– + æœºå™¨å­¦ä¹ èŠ‚ç‚¹çº§é¢„æµ‹"çš„åˆ›æ–°æ¶æ„ï¼Œå¹¶æä¾›ä¸“ä¸šçš„äº¤äº’å¼å¯è§†åŒ–ç•Œé¢ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- **ğŸ§  å¤šæ¨¡å‹æ”¯æŒ**ï¼šBPç¥ç»ç½‘ç»œã€SVMã€KNNã€çº¿æ€§å›å½’åŠé›†æˆæ–¹æ³•
- **ğŸ“Š å›¾ç¥ç»ç½‘ç»œ**ï¼šä½¿ç”¨GCNæå–èŠ‚ç‚¹é—´ç©ºé—´å…³ç³»ç‰¹å¾
- **ğŸ¨ äº¤äº’å¼å¯è§†åŒ–**ï¼šåŸºäºDashçš„ç°ä»£åŒ–Webç•Œé¢
- **âš¡ ä¸€é”®è¿è¡Œ**ï¼šè®­ç»ƒå®Œæˆè‡ªåŠ¨å¯åŠ¨å¯è§†åŒ–åº”ç”¨
- **ğŸ“ˆ å¤šæ­¥é¢„æµ‹**ï¼šæ”¯æŒ12æ­¥æ—¶é—´åºåˆ—é¢„æµ‹

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

```
traffic-prediction/
â”œâ”€â”€ ğŸ“ data/                    # æ•°æ®ç›®å½•
â”‚   â””â”€â”€ PEMS-08/
â”‚       â”œâ”€â”€ pems08.npz         # ä¸»æ•°æ®æ–‡ä»¶
â”‚       â””â”€â”€ distance.csv       # èŠ‚ç‚¹è·ç¦»çŸ©é˜µ
â”œâ”€â”€ ğŸ“ models/                  # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ base.py                # åŸºç¡€æ¨¡å‹ç±»
â”‚   â”œâ”€â”€ gcn.py                 # å›¾å·ç§¯ç½‘ç»œ
â”‚   â”œâ”€â”€ ğŸ“ ml_models/          # æœºå™¨å­¦ä¹ æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ bp.py              # BPç¥ç»ç½‘ç»œ
â”‚   â”‚   â”œâ”€â”€ svm.py             # æ”¯æŒå‘é‡æœº
â”‚   â”‚   â”œâ”€â”€ knn.py             # Kè¿‘é‚»
â”‚   â”‚   â””â”€â”€ linear.py          # çº¿æ€§å›å½’
â”‚   â””â”€â”€ ğŸ“ ensemble/           # é›†æˆå­¦ä¹ 
â”‚       â”œâ”€â”€ bagging.py         # Baggingé›†æˆ
â”‚       â”œâ”€â”€ adaboost.py        # AdaBoosté›†æˆ
â”‚       â””â”€â”€ stacking.py        # Stackingé›†æˆ
â”œâ”€â”€ ğŸ“ utils/                   # å·¥å…·æ¨¡å—
â”‚   â”œâ”€â”€ data_processor.py      # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ metrics.py             # è¯„ä¼°æŒ‡æ ‡
â”‚   â””â”€â”€ visualization.py       # å¯è§†åŒ–å·¥å…·
â”œâ”€â”€ ğŸ“ app/                     # Webå¯è§†åŒ–åº”ç”¨
â”‚   â”œâ”€â”€ app.py                 # Dashåº”ç”¨ä¸»æ–‡ä»¶
â”‚   â””â”€â”€ ğŸ“ data/               # å¯è§†åŒ–æ•°æ®
â”‚       â”œâ”€â”€ node_coords.csv    # èŠ‚ç‚¹åæ ‡
â”‚       â”œâ”€â”€ predictions.npy    # é¢„æµ‹ç»“æœ
â”‚       â””â”€â”€ y_true.npy         # çœŸå®å€¼
â”œâ”€â”€ train.py                   # è®­ç»ƒä¸»ç¨‹åº
â””â”€â”€ README.md                  # é¡¹ç›®æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- PyTorch 1.9+
- PyTorch Geometric
- scikit-learn
- Dash & Plotly
- NumPy & Pandas

### å®‰è£…ä¾èµ–

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n traffic-pred python=3.9
conda activate traffic-pred

# å®‰è£…PyTorch
conda install pytorch torchvision torchaudio -c pytorch

# å®‰è£…PyTorch Geometric
pip install torch-geometric

# å®‰è£…å…¶ä»–ä¾èµ–
pip install scikit-learn dash plotly pandas numpy networkx
```

### ä¸€é”®è¿è¡Œ

```bash
# è®­ç»ƒæ¨¡å‹å¹¶è‡ªåŠ¨å¯åŠ¨å¯è§†åŒ–
python train.py --model bp

# æµè§ˆå™¨è®¿é—®
# http://127.0.0.1:8050
```

## ğŸ® ä½¿ç”¨æŒ‡å—

### 1. æ¨¡å‹è®­ç»ƒ

æ”¯æŒå¤šç§é¢„æµ‹æ¨¡å‹ï¼š

```bash
# BPç¥ç»ç½‘ç»œï¼ˆæ¨èï¼‰
python train.py --model bp

# AdaBoosté›†æˆï¼ˆæ•ˆæœæœ€ä½³ï¼‰
python train.py --model adaboost

# Baggingé›†æˆ
python train.py --model bagging

# Stackingé›†æˆ
python train.py --model stacking

# å…¶ä»–å•ä¸€æ¨¡å‹
python train.py --model svm
python train.py --model knn
python train.py --model linear
```

### 2. è‡ªå®šä¹‰å‚æ•°

```bash
# è‡ªå®šä¹‰æ¨¡å‹å‚æ•°
python train.py --model bp --model_params '{"hidden_sizes": [128, 64], "learning_rate": 0.01}'

# è‡ªå®šä¹‰æ•°æ®å‚æ•°
python train.py --model bp --seq_len 24 --pred_len 6
```

### 3. å¯è§†åŒ–ç•Œé¢

è®­ç»ƒå®Œæˆåè‡ªåŠ¨å¯åŠ¨ï¼Œæˆ–æ‰‹åŠ¨å¯åŠ¨ï¼š

```bash
cd app
python app.py
```

**ç•Œé¢åŠŸèƒ½**ï¼š
- ğŸ—ºï¸ **èŠ‚ç‚¹åˆ†å¸ƒå›¾**ï¼šæ˜¾ç¤º170ä¸ªä¼ æ„Ÿå™¨èŠ‚ç‚¹çš„ç½‘ç»œæ‹“æ‰‘
- ğŸ“ˆ **é¢„æµ‹æ›²çº¿**ï¼šç‚¹å‡»èŠ‚ç‚¹æŸ¥çœ‹ç¬¬12æ­¥é¢„æµ‹vsçœŸå€¼å¯¹æ¯”
- ğŸ“Š **å®æ—¶ä¿¡æ¯**ï¼šæ˜¾ç¤ºèŠ‚ç‚¹åæ ‡ã€é¢„æµ‹æ­¥é•¿ç­‰ä¿¡æ¯

## ğŸ§® æŠ€æœ¯æ¶æ„

### æ•°æ®æµç¨‹

```
åŸå§‹æ•°æ® â†’ æ—¶é—´åºåˆ—åˆ‡ç‰‡ â†’ GCNç‰¹å¾æå– â†’ èŠ‚ç‚¹çº§é¢„æµ‹ â†’ ç»“æœå¯è§†åŒ–
```

### æ ¸å¿ƒç®—æ³•

1. **GCNç‰¹å¾æå–**
   - è¾“å…¥ï¼š`(batch, nodes, features)`
   - è¾“å‡ºï¼š`(batch, nodes, gcn_dim)`
   - æå–èŠ‚ç‚¹é—´ç©ºé—´å…³ç³»ç‰¹å¾

2. **èŠ‚ç‚¹çº§é¢„æµ‹**
   - è¾“å…¥ï¼š`(batch*nodes, gcn_dim)`
   - è¾“å‡ºï¼š`(batch*nodes, pred_len)`
   - æ¯ä¸ªèŠ‚ç‚¹ç‹¬ç«‹é¢„æµ‹12æ­¥

3. **åå½’ä¸€åŒ–**
   - åªå¯¹ç›®æ ‡ç‰¹å¾ï¼ˆäº¤é€šæµé‡ï¼‰è¿›è¡Œåå½’ä¸€åŒ–
   - ä¿æŒé¢„æµ‹ç»“æœçš„çœŸå®å°ºåº¦

## ğŸ”¬ ç®—æ³•è¯¦ç»†å®ç°

### 1. æ•°æ®é¢„å¤„ç†ç®—æ³•

#### æ—¶é—´åºåˆ—åˆ‡ç‰‡
```python
def create_sequences(data, seq_len, pred_len):
    """
    å°†åŸå§‹æ—¶é—´åºåˆ—æ•°æ®åˆ‡ç‰‡ä¸ºè®­ç»ƒæ ·æœ¬
    
    è¾“å…¥: data.shape = (T, N, F)  # T=æ—¶é—´æ­¥, N=èŠ‚ç‚¹æ•°, F=ç‰¹å¾æ•°
    è¾“å‡º: X.shape = (samples, seq_len, N, F)
         Y.shape = (samples, N, pred_len)
    """
    X, Y = [], []
    for i in range(len(data) - seq_len - pred_len + 1):
        # è¾“å…¥åºåˆ—ï¼šè¿‡å»12ä¸ªæ—¶é—´æ­¥çš„æ‰€æœ‰ç‰¹å¾
        x = data[i:i+seq_len]  # (12, 170, 3)
        # ç›®æ ‡åºåˆ—ï¼šæœªæ¥12ä¸ªæ—¶é—´æ­¥çš„æµé‡ç‰¹å¾
        y = data[i+seq_len:i+seq_len+pred_len, :, 0]  # (12, 170)
        X.append(x)
        Y.append(y.T)  # è½¬ç½®ä¸º (170, 12)
    return np.array(X), np.array(Y)
```

#### æ•°æ®å½’ä¸€åŒ–
```python
def normalize_data(data):
    """
    ä½¿ç”¨Z-scoreæ ‡å‡†åŒ–
    X_norm = (X - Î¼) / Ïƒ
    """
    mean = np.mean(data, axis=(0, 1), keepdims=True)
    std = np.std(data, axis=(0, 1), keepdims=True)
    normalized = (data - mean) / (std + 1e-8)
    return normalized, mean, std
```

### 2. å›¾å·ç§¯ç½‘ç»œ(GCN)å®ç°

#### ç½‘ç»œæ¶æ„
```python
class GCN(nn.Module):
    def __init__(self, input_dim=3, hidden_dim=64, output_dim=32, num_layers=2):
        super().__init__()
        self.layers = nn.ModuleList()
        
        # ç¬¬ä¸€å±‚ï¼šinput_dim -> hidden_dim
        self.layers.append(GCNConv(input_dim, hidden_dim))
        
        # ä¸­é—´å±‚ï¼šhidden_dim -> hidden_dim
        for _ in range(num_layers - 2):
            self.layers.append(GCNConv(hidden_dim, hidden_dim))
            
        # è¾“å‡ºå±‚ï¼šhidden_dim -> output_dim
        self.layers.append(GCNConv(hidden_dim, output_dim))
        
        self.dropout = nn.Dropout(0.2)
        self.activation = nn.ReLU()
```

#### GCNå‰å‘ä¼ æ’­
```python
def forward(self, x, edge_index):
    """
    GCNå‰å‘ä¼ æ’­è¿‡ç¨‹
    
    è¾“å…¥: x.shape = (batch_size, num_nodes, input_features)
         edge_index.shape = (2, num_edges)
    è¾“å‡º: x.shape = (batch_size, num_nodes, output_features)
    """
    batch_size, num_nodes, _ = x.shape
    
    # é‡å¡‘ä¸º (batch_size * num_nodes, features)
    x = x.view(-1, x.size(-1))
    
    # æ‰©å±•è¾¹ç´¢å¼•ä»¥å¤„ç†æ‰¹æ¬¡æ•°æ®
    edge_indices = []
    for i in range(batch_size):
        edge_indices.append(edge_index + i * num_nodes)
    batch_edge_index = torch.cat(edge_indices, dim=1)
    
    # é€å±‚ä¼ æ’­
    for i, layer in enumerate(self.layers):
        x = layer(x, batch_edge_index)
        if i < len(self.layers) - 1:
            x = self.activation(x)
            x = self.dropout(x)
    
    # é‡å¡‘å› (batch_size, num_nodes, output_features)
    return x.view(batch_size, num_nodes, -1)
```

#### å›¾æ„å»ºç®—æ³•
```python
def build_graph_from_distance(distance_matrix, threshold=0.1):
    """
    åŸºäºè·ç¦»çŸ©é˜µæ„å»ºå›¾çš„é‚»æ¥å…³ç³»
    
    ä½¿ç”¨é«˜æ–¯æ ¸å‡½æ•°: w_ij = exp(-d_ijÂ²/ÏƒÂ²)
    """
    # è®¡ç®—é«˜æ–¯æƒé‡
    sigma = np.std(distance_matrix)
    weights = np.exp(-distance_matrix**2 / sigma**2)
    
    # è®¾ç½®é˜ˆå€¼ï¼Œè¿‡æ»¤å¼±è¿æ¥
    adjacency = (weights > threshold).astype(float)
    
    # è½¬æ¢ä¸ºè¾¹ç´¢å¼•æ ¼å¼
    edge_index = np.array(np.nonzero(adjacency))
    return torch.LongTensor(edge_index)
```

### 3. æœºå™¨å­¦ä¹ æ¨¡å‹å®ç°

#### BPç¥ç»ç½‘ç»œ
```python
class MultiOutputBP(BaseModel):
    def __init__(self, input_size, hidden_sizes=[64, 32], learning_rate=0.001):
        super().__init__('MultiOutputBP')
        self.network = self._build_network(input_size, hidden_sizes, 12)
        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)
        self.criterion = nn.MSELoss()
    
    def _build_network(self, input_size, hidden_sizes, output_size):
        """æ„å»ºå¤šå±‚æ„ŸçŸ¥æœºç½‘ç»œ"""
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            prev_size = hidden_size
        
        layers.append(nn.Linear(prev_size, output_size))
        return nn.Sequential(*layers)
    
    def fit(self, X, y, epochs=100, batch_size=32):
        """è®­ç»ƒè¿‡ç¨‹"""
        dataset = TensorDataset(torch.FloatTensor(X), torch.FloatTensor(y))
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        self.network.train()
        for epoch in range(epochs):
            total_loss = 0
            for batch_X, batch_y in dataloader:
                self.optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                predictions = self.network(batch_X)
                loss = self.criterion(predictions, batch_y)
                
                # åå‘ä¼ æ’­
                loss.backward()
                self.optimizer.step()
                
                total_loss += loss.item()
            
            if epoch % 20 == 0:
                print(f'Epoch {epoch}, Loss: {total_loss/len(dataloader):.4f}')
```

#### é›†æˆå­¦ä¹ ç®—æ³•

##### AdaBoostå®ç°
```python
def fit(self, X, y):
    """AdaBoostè®­ç»ƒç®—æ³•"""
    n_samples = len(X)
    # åˆå§‹åŒ–æ ·æœ¬æƒé‡
    sample_weights = np.ones(n_samples) / n_samples
    
    for i in range(self.n_estimators):
        # è®­ç»ƒåŸºæ¨¡å‹
        model = self.base_model_class(**self.base_params)
        model.fit(X, y, sample_weight=sample_weights)
        
        # è®¡ç®—é¢„æµ‹è¯¯å·®
        predictions = model.predict(X)
        errors = np.abs(predictions - y)
        
        # å¤šè¾“å‡ºå¤„ç†ï¼šå¯¹æ‰€æœ‰è¾“å‡ºç»´åº¦æ±‚å¹³å‡
        if errors.ndim > 1:
            errors = np.mean(errors, axis=1)
        
        # è®¡ç®—åŠ æƒè¯¯å·®ç‡
        weighted_error = np.sum(sample_weights * errors) / np.sum(sample_weights)
        
        # æ—©åœæ¡ä»¶
        if weighted_error >= 0.5:
            break
        
        # è®¡ç®—æ¨¡å‹æƒé‡
        alpha = self.learning_rate * 0.5 * np.log((1 - weighted_error) / weighted_error)
        
        # æ›´æ–°æ ·æœ¬æƒé‡
        sample_weights *= np.exp(alpha * errors)
        sample_weights /= np.sum(sample_weights)  # å½’ä¸€åŒ–
        
        self.models.append(model)
        self.weights.append(alpha)
```

##### Stackingå®ç°
```python
def fit(self, X, y):
    """Stackingä¸¤å±‚è®­ç»ƒ"""
    # ç¬¬ä¸€å±‚ï¼šè®­ç»ƒåŸºæ¨¡å‹
    meta_features = []
    for model_class in self.base_models:
        model = model_class()
        model.fit(X, y)
        
        # ç”Ÿæˆå…ƒç‰¹å¾ï¼ˆäº¤å‰éªŒè¯é¢„æµ‹ï¼‰
        predictions = model.predict(X)
        meta_features.append(predictions)
        self.trained_base_models.append(model)
    
    # æ‹¼æ¥å…ƒç‰¹å¾
    meta_X = np.column_stack(meta_features)  # (samples, n_models * pred_len)
    
    # ç¬¬äºŒå±‚ï¼šè®­ç»ƒå…ƒæ¨¡å‹
    self.trained_meta_model = self.meta_model(**self.meta_params)
    self.trained_meta_model.fit(meta_X, y)
```

### 4. è®­ç»ƒæµç¨‹ç®—æ³•

#### å®Œæ•´è®­ç»ƒPipeline
```python
def train_pipeline(data_path, model_name, seq_len=12, pred_len=12):
    """å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
    
    # 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
    processor = DataProcessor(data_path, seq_len, pred_len)
    (X_train, Y_train), (X_val, Y_val), (X_test, Y_test), adj = processor.prepare_data()
    
    # 2. æ„å»ºå›¾ç»“æ„
    edge_index = adj_to_edge_index(adj)
    
    # 3. GCNç‰¹å¾æå–
    gcn_features_train = processor.extract_gcn_features(X_train[:, -1, :, :], edge_index)
    gcn_features_test = processor.extract_gcn_features(X_test[:, -1, :, :], edge_index)
    
    # 4. æ•°æ®é‡å¡‘ï¼šèŠ‚ç‚¹çº§é¢„æµ‹
    n_samples, n_nodes, gcn_dim = gcn_features_train.shape
    X_train_flat = gcn_features_train.reshape(n_samples * n_nodes, gcn_dim)
    Y_train_flat = Y_train.reshape(n_samples * n_nodes, pred_len)
    
    # 5. æ¨¡å‹è®­ç»ƒ
    model = get_model(model_name, input_size=gcn_dim)
    model.fit(X_train_flat, Y_train_flat)
    
    # 6. é¢„æµ‹ä¸è¯„ä¼°
    predictions = model.predict(X_test_flat)
    predictions = predictions.reshape(n_test_samples, n_nodes, pred_len)
    
    # 7. åå½’ä¸€åŒ–
    predictions = processor.inverse_normalize_target(predictions)
    y_true = processor.inverse_normalize_target(Y_test)
    
    return predictions, y_true
```

### 5. è¯„ä¼°æŒ‡æ ‡ç®—æ³•

#### å¤šç»´åº¦è¯„ä¼°
```python
def calculate_comprehensive_metrics(y_true, y_pred):
    """è®¡ç®—å¤šç»´åº¦è¯„ä¼°æŒ‡æ ‡"""
    
    # æ•´ä½“æŒ‡æ ‡
    mae = np.mean(np.abs(y_true - y_pred))
    mse = np.mean((y_true - y_pred) ** 2)
    rmse = np.sqrt(mse)
    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
    
    # èŠ‚ç‚¹çº§æŒ‡æ ‡
    node_metrics = {}
    for node in range(y_true.shape[1]):
        node_mae = np.mean(np.abs(y_true[:, node, :] - y_pred[:, node, :]))
        node_metrics[f'node_{node}_mae'] = node_mae
    
    # æ—¶é—´æ­¥çº§æŒ‡æ ‡
    horizon_metrics = {}
    for step in range(y_true.shape[2]):
        step_mae = np.mean(np.abs(y_true[:, :, step] - y_pred[:, :, step]))
        horizon_metrics[f'step_{step+1}_mae'] = step_mae
    
    return {
        'overall': {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'MAPE': mape},
        'by_node': node_metrics,
        'by_horizon': horizon_metrics
    }
```

### 6. ç®—æ³•ä¼˜åŒ–ç­–ç•¥

#### å†…å­˜ä¼˜åŒ–
- **æ‰¹æ¬¡å¤„ç†**ï¼šé™åˆ¶è®­ç»ƒæ ·æœ¬æ•°é‡ä¸º2000ï¼Œé¿å…å†…å­˜æº¢å‡º
- **æ¢¯åº¦ç´¯ç§¯**ï¼šå¯¹äºå¤§æ‰¹æ¬¡æ•°æ®ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æŠ€æœ¯
- **ç‰¹å¾ç¼“å­˜**ï¼šç¼“å­˜GCNæå–çš„ç‰¹å¾ï¼Œé¿å…é‡å¤è®¡ç®—

#### è®¡ç®—ä¼˜åŒ–
- **å¹¶è¡Œè®¡ç®—**ï¼šåˆ©ç”¨PyTorchçš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›
- **GPUåŠ é€Ÿ**ï¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨CUDAåŠ é€Ÿ
- **æ—©åœæœºåˆ¶**ï¼šåœ¨éªŒè¯é›†ä¸Šç›‘æ§æ€§èƒ½ï¼Œé¿å…è¿‡æ‹Ÿåˆ

#### æ•°å€¼ç¨³å®šæ€§
- **æ¢¯åº¦è£å‰ª**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- **æƒé‡åˆå§‹åŒ–**ï¼šä½¿ç”¨Xavieråˆå§‹åŒ–
- **æ‰¹å½’ä¸€åŒ–**ï¼šåœ¨æ·±å±‚ç½‘ç»œä¸­ä½¿ç”¨BatchNorm

## ğŸ“Š æ¨¡å‹æ€§èƒ½

| æ¨¡å‹ | MAE | MSE | RMSE | ç‰¹ç‚¹ |
|------|-----|-----|------|------|
| **AdaBoost** | 87.19 | 20723.02 | 143.95 | ğŸ¥‡ æ•ˆæœæœ€ä½³ |
| **Bagging** | 89.34 | 17602.97 | 132.68 | ğŸ¥ˆ ç¨³å®šæ€§å¥½ |
| **BPç¥ç»ç½‘ç»œ** | 97.92 | 21297.35 | 145.94 | ğŸ¥‰ æ”¶æ•›å¿«é€Ÿ |
| **Stacking** | 100.60 | 23680.56 | 153.88 | ğŸ… å¤æ‚åº¦é«˜ |

## ğŸ”§ é…ç½®è¯´æ˜

### æ•°æ®é…ç½®

- **åºåˆ—é•¿åº¦**ï¼š`seq_len=12`ï¼ˆè¾“å…¥12ä¸ªæ—¶é—´æ­¥ï¼‰
- **é¢„æµ‹é•¿åº¦**ï¼š`pred_len=12`ï¼ˆé¢„æµ‹12ä¸ªæ—¶é—´æ­¥ï¼‰
- **èŠ‚ç‚¹æ•°é‡**ï¼š170ä¸ªäº¤é€šä¼ æ„Ÿå™¨
- **ç‰¹å¾ç»´åº¦**ï¼š3ï¼ˆæµé‡ã€å æœ‰ç‡ã€é€Ÿåº¦ï¼‰

### æ¨¡å‹é…ç½®

- **GCNç»´åº¦**ï¼š32ç»´ç‰¹å¾å‘é‡
- **è®­ç»ƒæ ·æœ¬**ï¼šé™åˆ¶2000ä¸ªï¼ˆåŠ é€Ÿè°ƒè¯•ï¼‰
- **è®¾å¤‡æ”¯æŒ**ï¼šè‡ªåŠ¨æ£€æµ‹CUDA/CPU

## ğŸ“ æ•°æ®è¯´æ˜

### PEMS-08æ•°æ®é›†

- **æ¥æº**ï¼šåŠ å·äº¤é€šç®¡ç†ç³»ç»Ÿ
- **æ—¶é—´èŒƒå›´**ï¼š2016å¹´7æœˆ-8æœˆ
- **é‡‡æ ·é¢‘ç‡**ï¼š5åˆ†é’Ÿé—´éš”
- **èŠ‚ç‚¹æ•°é‡**ï¼š170ä¸ªæ£€æµ‹å™¨
- **æ•°æ®æ ¼å¼**ï¼š`(æ—¶é—´æ­¥, èŠ‚ç‚¹, ç‰¹å¾)`

## ğŸ¨ å¯è§†åŒ–ç‰¹æ€§

### ç•Œé¢è®¾è®¡

- **ç°ä»£åŒ–UI**ï¼šå¡ç‰‡å¼å¸ƒå±€ï¼Œè“ç°é…è‰²
- **å“åº”å¼è®¾è®¡**ï¼šå·¦å³åˆ†æ ï¼Œ48%å®½åº¦å¸ƒå±€
- **äº¤äº’ä½“éªŒ**ï¼šç‚¹å‡»èŠ‚ç‚¹å³æ—¶æ˜¾ç¤ºé¢„æµ‹æ›²çº¿
- **çŠ¶æ€æç¤º**ï¼šè‡ªåŠ¨è¯†åˆ«çœŸå®/æ¨¡æ‹Ÿæ•°æ®

### å›¾è¡¨åŠŸèƒ½

- **èŠ‚ç‚¹åˆ†å¸ƒ**ï¼šåŸºäºè·ç¦»å…³ç³»çš„ç½‘ç»œæ‹“æ‰‘å¯è§†åŒ–
- **é¢„æµ‹å¯¹æ¯”**ï¼šçœŸå€¼vsé¢„æµ‹å€¼æ›²çº¿å¯¹æ¯”
- **æ‚¬åœä¿¡æ¯**ï¼šè¯¦ç»†çš„æ•°å€¼å’Œåæ ‡ä¿¡æ¯

## ğŸ› ï¸ å¼€å‘æŒ‡å—

### æ·»åŠ æ–°æ¨¡å‹

1. åœ¨`models/ml_models/`åˆ›å»ºæ–°æ¨¡å‹æ–‡ä»¶
2. ç»§æ‰¿`BaseModel`ç±»
3. å®ç°`fit`å’Œ`predict`æ–¹æ³•
4. åœ¨`train.py`ä¸­æ³¨å†Œæ¨¡å‹

### è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡

åœ¨`utils/metrics.py`ä¸­æ·»åŠ æ–°çš„è¯„ä¼°å‡½æ•°ï¼š

```python
def custom_metric(y_true, y_pred):
    # è‡ªå®šä¹‰æŒ‡æ ‡è®¡ç®—
    return metric_value
```

## ğŸ› å¸¸è§é—®é¢˜

### Q: è®­ç»ƒæ—¶æ˜¾ç¤ºCUDA out of memoryï¼Ÿ
A: å‡å°‘`max_train`å‚æ•°æˆ–ä½¿ç”¨CPUè®­ç»ƒ

### Q: å¯è§†åŒ–æ˜¾ç¤ºæ¨¡æ‹Ÿæ•°æ®ï¼Ÿ
A: ç¡®ä¿å…ˆè¿è¡Œ`train.py`ç”Ÿæˆé¢„æµ‹æ•°æ®

### Q: é›†æˆæ¨¡å‹è®­ç»ƒå¾ˆæ…¢ï¼Ÿ
A: å‡å°‘`n_estimators`å‚æ•°æˆ–ä½¿ç”¨æ›´ç®€å•çš„åŸºæ¨¡å‹

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ‘¥ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ªStarï¼ 