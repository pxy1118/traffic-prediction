# 第2章  基础学习器理论与实现

## 2.1 图卷积网络特征提取器

### 2.1.1 图卷积网络基本原理

图卷积网络（Graph Convolutional Network, GCN）是一种专门处理图结构数据的深度学习模型。在交通流预测任务中，交通网络天然具有图结构特征，其中交通传感器节点通过道路连接形成复杂的空间拓扑关系。

**图的数学表示：**
设交通网络图为 $G = (V, E, A)$，其中：
- $V = \{v_1, v_2, ..., v_N\}$ 表示 $N$ 个交通传感器节点
- $E$ 表示节点间的边集合，反映道路连接关系
- $A \in \mathbb{R}^{N \times N}$ 表示邻接矩阵，$A_{ij} = 1$ 表示节点 $i$ 和 $j$ 相连

**图卷积操作：**
对于节点特征矩阵 $X \in \mathbb{R}^{N \times F}$，图卷积层的前向传播定义为：

$$H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})$$

其中：
- $\tilde{A} = A + I$ 为添加自环的邻接矩阵
- $\tilde{D}$ 为度矩阵，$\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$
- $W^{(l)}$ 为第 $l$ 层的可训练权重矩阵
- $\sigma(\cdot)$ 为激活函数

### 2.1.2 GCN特征提取器设计

本项目实现的GCN特征提取器具有以下特点：

**网络架构：**
```python
class GCN(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2):
        # 输入层：in_channels → hidden_channels
        # 隐藏层：hidden_channels → hidden_channels
        # 输出层：hidden_channels → out_channels
```

**关键技术特性：**
1) **多层图卷积**：采用2层图卷积结构，第一层提取局部特征，第二层聚合更大范围的空间信息
2) **ReLU激活函数**：在隐藏层使用ReLU激活函数，增强模型的非线性表达能力
3) **Dropout正则化**：采用0.2的dropout率防止过拟合，提升模型泛化性能
4) **自适应特征维度**：支持32维输出特征，为后续机器学习算法提供紧凑而丰富的表示

**特征提取流程：**
- **输入**：交通传感器节点的原始特征（流量、速度、占有率）
- **图卷积**：通过邻接矩阵聚合邻居节点信息
- **输出**：每个节点的32维嵌入向量

## 2.2 传统机器学习算法

### 2.2.1 支持向量机回归（SVR）

支持向量机回归是一种基于统计学习理论的非线性回归方法，特别适合处理高维数据的非线性关系。

**基本原理：**
SVR通过寻找一个超平面来拟合数据，目标是找到一个函数 $f(x) = w^T\phi(x) + b$，使得：

$$\min_{w,b,\xi,\xi^*} \frac{1}{2}||w||^2 + C\sum_{i=1}^n(\xi_i + \xi_i^*)$$

约束条件：
$$y_i - w^T\phi(x_i) - b \leq \epsilon + \xi_i$$
$$w^T\phi(x_i) + b - y_i \leq \epsilon + \xi_i^*$$

**参数设置：**
- **核函数**：采用RBF（径向基函数）核，$K(x_i, x_j) = \exp(-\gamma||x_i - x_j||^2)$
- **正则化参数C**：默认值1.0，控制模型复杂度与拟合精度的平衡
- **损失函数参数ε**：默认值0.1，定义ε-不敏感损失函数的容忍度

**多输出处理：**
采用多个独立SVR模型分别预测未来12个时间步的交通流量，每个时间步对应一个专门的SVR模型。

### 2.2.2 K近邻回归（KNN）

K近邻算法是一种基于实例的懒惰学习算法，通过寻找训练集中最相似的K个样本来进行预测。

**算法原理：**
对于测试样本 $x$，KNN算法的预测值计算为：

$$\hat{y} = \frac{1}{K}\sum_{i=1}^K y_i$$

其中 $y_i$ 是K个最近邻样本的目标值。

**关键参数：**
- **邻居数量K**：默认值5，影响模型的偏差-方差权衡
- **权重策略**：采用'uniform'均匀权重，所有邻居具有相同影响力
- **距离度量**：使用欧几里得距离计算样本间相似性

**优势特点：**
1) **简单有效**：算法简单，易于理解和实现
2) **非参数模型**：不对数据分布做假设，适应性强
3) **局部性质**：能够捕获数据的局部模式和特征

### 2.2.3 线性回归

线性回归是最基础的回归算法，通过建立特征与目标变量间的线性关系进行预测。

**数学模型：**
$$y = X\beta + \epsilon$$

其中：
- $y \in \mathbb{R}^{n \times p}$ 为多输出目标矩阵
- $X \in \mathbb{R}^{n \times d}$ 为特征矩阵
- $\beta \in \mathbb{R}^{d \times p}$ 为回归系数矩阵
- $\epsilon$ 为误差项

**参数估计：**
采用最小二乘法求解，回归系数的闭式解为：
$$\hat{\beta} = (X^TX)^{-1}X^Ty$$

**模型特点：**
- **可解释性强**：每个特征的影响可以通过回归系数直接解释
- **计算效率高**：训练和预测速度快，适合大规模数据
- **基准模型**：作为其他复杂模型的性能比较基准

### 2.2.4 BP神经网络

反向传播神经网络（BP Neural Network）是一种多层前馈神经网络，通过反向传播算法训练网络参数。

**网络架构：**
- **输入层**：接受32维GCN特征
- **隐藏层**：采用两层全连接结构（64→32），使用ReLU激活函数
- **输出层**：输出12维预测结果，对应未来12个时间步

**关键技术：**
1) **激活函数**：隐藏层使用ReLU函数，输出层使用线性激活
2) **正则化**：采用0.2的Dropout率防止过拟合
3) **优化算法**：使用Adam优化器，学习率0.001
4) **损失函数**：均方误差损失，适合回归任务

**训练策略：**
- **批处理训练**：批大小32，平衡训练效率与内存使用
- **训练轮数**：默认100个epoch，可根据验证集性能调整
- **设备自适应**：支持GPU加速训练，提升计算效率

## 2.3 集成学习方法

### 2.3.1 Bagging集成

Bagging（Bootstrap Aggregating）通过构建多个基学习器并平均它们的预测结果来提升模型性能。

**算法流程：**
1) **Bootstrap采样**：从训练集中有放回地采样生成多个子数据集
2) **并行训练**：在每个子数据集上独立训练基学习器
3) **聚合预测**：对所有基学习器的预测结果取平均

**数学表达：**
$$\hat{y}_{bag} = \frac{1}{M}\sum_{m=1}^M \hat{y}_m$$

其中 $M$ 为基学习器数量，$\hat{y}_m$ 为第 $m$ 个基学习器的预测。

**实现特点：**
- **基学习器**：默认使用BP神经网络作为基学习器
- **集成数量**：15个基学习器，平衡性能提升与计算成本
- **方差减少**：通过平均化减少模型方差，提升预测稳定性

### 2.3.2 AdaBoost集成

AdaBoost（Adaptive Boosting）是一种自适应增强算法，通过迭代训练弱学习器并调整样本权重来构建强学习器。

**算法原理：**
1) **初始化权重**：为每个训练样本分配相等的初始权重
2) **迭代训练**：训练弱学习器，增加错误样本权重
3) **加权组合**：根据学习器性能确定权重，组合所有弱学习器

**权重更新规则：**
- 学习器权重：$\alpha_m = \frac{1}{2}\ln\frac{1-\epsilon_m}{\epsilon_m}$
- 样本权重：$w_i^{(m+1)} = w_i^{(m)}\exp(-\alpha_m y_i h_m(x_i))$

其中 $\epsilon_m$ 为第 $m$ 个学习器的错误率。

**适用特点：**
- **基学习器**：使用线性回归作为弱学习器
- **自适应性**：能够自动关注困难样本，提升整体性能
- **偏差减少**：通过boosting机制减少模型偏差

### 2.3.3 Stacking集成

Stacking（Stacked Generalization）通过训练元学习器来组合多个基学习器的预测结果。

**两层架构：**
1) **第一层（基学习器层）**：
   - 线性回归：提供线性基准预测
   - K近邻：捕获局部模式
   
2) **第二层（元学习器层）**：
   - 线性回归作为元学习器，学习如何最优组合基学习器

**训练流程：**
1) **基学习器训练**：在训练集上训练所有基学习器
2) **元特征生成**：基学习器预测结果作为元特征
3) **元学习器训练**：在元特征上训练元学习器

**优势特点：**
- **异构集成**：组合不同类型的算法，发挥各自优势
- **学习组合**：通过元学习器自动学习最优组合策略
- **性能提升**：通常能获得比单一模型更好的预测性能

## 2.4 模型统一接口设计

### 2.4.1 基础模型抽象类

本项目设计了统一的BaseModel抽象类，为所有学习器提供标准接口：

```python
class BaseModel:
    def fit(self, X: np.ndarray, y: np.ndarray, **kwargs) -> None:
        """训练模型"""
        
    def predict(self, X: np.ndarray) -> np.ndarray:
        """预测结果"""
        
    def get_params(self) -> dict:
        """获取模型参数"""
        
    def set_params(self, **params) -> None:
        """设置模型参数"""
```

### 2.4.2 多输出适配机制

针对交通流预测的多步预测需求，所有学习器都实现了多输出回归功能：

1) **SVR**：使用多个独立的SVR模型，每个输出一个SVR
2) **KNN/线性回归**：sklearn原生支持多输出回归
3) **BP神经网络**：输出层设计为12个神经元，对应12个预测步长
4) **集成方法**：继承基学习器的多输出能力

### 2.4.3 统一训练框架

所有模型遵循相同的训练和预测流程：
- **数据输入**：32维GCN特征作为输入
- **目标输出**：12维交通流量预测向量
- **性能评估**：使用MAE、MSE、RMSE等统一指标
- **模型保存**：支持模型持久化和加载功能

这种统一的设计使得不同算法之间能够公平比较，并为集成学习提供了便利的接口。